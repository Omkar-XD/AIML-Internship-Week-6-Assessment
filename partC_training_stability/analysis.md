• High learning rate causes divergence due to unstable updates  
• BatchNorm stabilizes gradients  
• Dropout lowers training accuracy but improves validation