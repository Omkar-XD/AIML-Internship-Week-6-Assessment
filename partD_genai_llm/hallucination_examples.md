Hallucination 1: Model invented fake research references

Reason: LLMs predict text, not facts

Mitigation: RAG, verification, constrained decoding